# Jira Stories for Code Quality and Performance Issues

Generated by Agent-1 Code Analysis, Agent-4 Integration Analysis, and Agent-5 Performance Analysis on 2025-08-31

## Critical Priority Stories

### BMP-CRITICAL-001: Replace hardcoded database paths with configurable paths
**Epic**: Infrastructure Configuration
**Story Points**: 3
**Priority**: Critical
**Labels**: `infrastructure`, `configuration`, `deployment`

**Description:**
The orchestration service has hardcoded database paths that will cause deployment issues across environments.

**Acceptance Criteria:**
- [ ] Replace hardcoded `self.base_path / 'dbs' / 'memory.duckdb'` with environment variable
- [ ] Add `DUCKDB_PATH` environment variable support with fallback default
- [ ] Update all database path references to use configuration
- [ ] Test deployment across dev/staging/prod environments
- [ ] Update documentation with new environment variables

**Technical Details:**
- **File**: `orchestrate_biological_memory.py:138`
- **Current**: `db_path = self.base_path / 'dbs' / 'memory.duckdb'`
- **Expected**: `db_path = os.getenv('DUCKDB_PATH', str(self.base_path / 'dbs' / 'memory.duckdb'))`

**Definition of Done:**
- Database paths are configurable via environment variables
- All environments can be deployed with different database paths
- No hardcoded paths remain in production code
- Integration tests pass with custom database paths

---

### BMP-CRITICAL-002: Implement HTTP connection pooling for LLM API
**Epic**: Performance Optimization
**Story Points**: 5
**Priority**: Critical
**Labels**: `performance`, `llm-integration`, `http-client`

**Description:**
The LLM integration service uses a single HTTP session without connection pooling, causing 100-300ms overhead per request that violates biological timing constraints (<50ms working memory target).

**Acceptance Criteria:**
- [ ] Replace single HTTP session with connection pool (min 5, max 20 connections)
- [ ] Implement keep-alive connections with connection reuse
- [ ] Add connection health checking and automatic recovery
- [ ] Measure and verify <10ms connection overhead
- [ ] Add metrics for connection pool utilization

**Technical Details:**
- **File**: `llm_integration_service.py:78-88`
- **Current**: Single requests.Session() with basic retry
- **Target**: HTTPConnectionPool with keep-alive and connection reuse
- **Performance Impact**: 300ms → 10ms connection overhead

**Definition of Done:**
- HTTP connection pool implemented with proper configuration
- Connection overhead reduced to <10ms
- Performance tests show 95% latency improvement
- Connection pool metrics integrated into monitoring

---

### BMP-CRITICAL-003: Reduce LLM API timeouts for biological timing
**Epic**: Performance Optimization
**Story Points**: 3
**Priority**: Critical  
**Labels**: `performance`, `llm-integration`, `timeout-optimization`

**Description:**
Current 300-second LLM API timeout exceeds biological timing targets by 6000× (50ms working memory, 500ms consolidation), risking pipeline hangs.

**Acceptance Criteria:**
- [ ] Implement tiered timeouts: 5s for working memory, 30s for consolidation
- [ ] Add circuit breaker pattern for timeout failures
- [ ] Implement fast-fail for non-critical operations
- [ ] Add timeout monitoring and alerting
- [ ] Test timeout behavior under load

**Technical Details:**
- **File**: `llm_integration_service.py:193-266`
- **Current**: 300-second default timeout
- **Target**: 5s (working memory), 30s (consolidation), 60s (analytics)
- **Risk Mitigation**: Circuit breaker prevents cascade failures

**Definition of Done:**
- Tiered timeout system implemented and configured
- Circuit breaker prevents timeout cascade failures  
- 99% of operations complete within biological timing targets
- Monitoring alerts on timeout threshold breaches

---

## High Priority Stories

### BMP-HIGH-007: Fix broken table references in long-term memory model
**Epic**: Data Models
**Story Points**: 5
**Priority**: High
**Labels**: `sql`, `data-model`, `bug-fix`

**Description:**
Long-term memory SQL model references tables that don't exist, causing compilation failures.

**Acceptance Criteria:**
- [ ] Remove or fix references to `semantic_associations` table
- [ ] Remove or fix references to `network_centrality` table  
- [ ] Create placeholder tables if functionality is needed
- [ ] Update model to handle missing associations gracefully
- [ ] Add proper error handling for missing dependencies
- [ ] Validate model compiles and runs successfully

**Technical Details:**
- **File**: `models/long_term_memory/stable_memories.sql:57-60`
- **Issue**: LEFT JOINs to non-existent tables causing SQL compilation failures
- **Impact**: Long-term memory processing completely broken

**Definition of Done:**
- Long-term memory model compiles without errors
- Model handles missing associations with null-safe defaults
- Pipeline runs end-to-end without table reference errors
- Unit tests validate model behavior with missing tables

### BMP-HIGH-008: Eliminate N+1 query pattern in memory processing
**Epic**: Performance Optimization  
**Story Points**: 8
**Priority**: High
**Labels**: `performance`, `n-plus-one`, `llm-batching`

**Description:**
Memory processing makes individual LLM API calls for each memory instead of batch processing, causing linear scaling that breaks biological timing constraints.

**Acceptance Criteria:**
- [ ] Implement batch LLM processing for memory groups (5-10 memories per batch)
- [ ] Reduce individual API calls to single batch requests
- [ ] Add batch response parsing and distribution logic  
- [ ] Measure and verify 90%+ latency reduction
- [ ] Implement fallback to individual processing for batch failures
- [ ] Add batch size optimization based on system performance

**Technical Details:**
- **File**: `stm_hierarchical_episodes.sql` and related LLM processing models
- **Current**: Individual LLM call per memory (10 memories = 10× latency) 
- **Target**: Batch processing (10 memories = 1.2× latency)
- **Performance Impact**: 10 memories: 30s → 3s processing time

**Definition of Done:**
- Batch LLM processing implemented across memory pipeline
- Linear scaling eliminated (O(n) → O(log n) for LLM calls)
- 90%+ reduction in total LLM processing time
- Biological timing targets achievable with realistic memory volumes

---

### BMP-HIGH-009: Add comprehensive SQL input validation
**Epic**: Security & Safety
**Story Points**: 8
**Priority**: High
**Labels**: `security`, `sql-injection`, `validation`

**Description:**
SQL runtime safety system only performs basic validation, missing potential security and stability issues.

**Acceptance Criteria:**
- [ ] Implement SQL AST parsing for structure validation
- [ ] Add query complexity limits (joins, subqueries, etc.)
- [ ] Validate parameter binding against SQL injection patterns
- [ ] Add query plan analysis for performance prediction
- [ ] Create validation rules for biological memory constraints
- [ ] Add comprehensive test suite for edge cases

**Technical Details:**
- **File**: `sql_runtime_safety.py:369-374`
- **Current**: Basic empty query check only
- **Needed**: Full SQL structure and security validation

**Definition of Done:**
- Comprehensive SQL validation prevents malformed queries
- Security vulnerabilities are blocked before execution
- Performance-dangerous queries are identified and rejected
- Validation rules are configurable per safety level

### BMP-HIGH-010: Increase database connection pool capacity
**Epic**: Performance Optimization
**Story Points**: 3
**Priority**: High  
**Labels**: `performance`, `connection-pool`, `resource-management`

**Description:**
Current connection pool limit (max_connections_per_db=10) insufficient for parallel biological cycles, risking connection exhaustion and pipeline bottlenecks.

**Acceptance Criteria:**
- [ ] Increase connection pool to 25-50 connections per database
- [ ] Add dynamic connection scaling based on system load
- [ ] Implement connection health monitoring and alerting
- [ ] Add circuit breaker for connection pool exhaustion
- [ ] Test parallel processing of working memory, consolidation, analytics
- [ ] Monitor connection utilization and queue wait times

**Technical Details:**
- **File**: `sql_runtime_safety.py:77-83`
- **Current**: max_connections_per_db=10
- **Target**: 25-50 connections with dynamic scaling
- **Risk**: Working memory + consolidation + analytics compete for connections

**Definition of Done:**
- Connection pool supports parallel biological processing cycles
- No connection exhaustion during peak memory processing
- Connection monitoring integrated with performance dashboard
- Circuit breaker prevents cascade failures on pool exhaustion

---

### BMP-HIGH-011: Implement LLM operation timeout and circuit breakers
**Epic**: LLM Integration
**Story Points**: 5
**Priority**: High
**Labels**: `llm`, `timeout`, `reliability`

**Description:**
Complex nested LLM calls in hierarchical episodes model lack timeout handling and could deadlock the pipeline.

**Acceptance Criteria:**
- [ ] Add timeout configuration for all LLM operations
- [ ] Implement circuit breaker pattern for LLM calls
- [ ] Add fallback mechanisms when LLM is unavailable
- [ ] Provide graceful degradation for LLM timeouts
- [ ] Monitor and alert on LLM operation failures
- [ ] Add retry logic with exponential backoff

**Technical Details:**
- **File**: `models/short_term_memory/stm_hierarchical_episodes.sql:180-323`
- **Risk**: Pipeline hangs on LLM timeouts, resource exhaustion
- **Solution**: Circuit breakers, timeouts, and fallback strategies

**Definition of Done:**
- LLM operations never hang the pipeline
- Circuit breakers activate after failure threshold
- Fallback mechanisms maintain pipeline functionality
- Monitoring shows LLM operation health status

### BMP-HIGH-010: Fix non-existent field references in working memory
**Epic**: Data Models  
**Story Points**: 2
**Priority**: High
**Labels**: `sql`, `data-model`, `bug-fix`

**Description:**
Working memory model references `previous_strength` field that doesn't exist in source data.

**Acceptance Criteria:**
- [ ] Identify source of `previous_strength` field requirement
- [ ] Either add field to source data or remove dependency
- [ ] Update Hebbian strength calculation logic
- [ ] Ensure working memory model compiles and runs
- [ ] Add proper null handling for missing fields
- [ ] Update documentation for field requirements

**Technical Details:**
- **File**: `models/working_memory/wm_active_context.sql:61`
- **Issue**: Reference to undefined `previous_strength` field
- **Impact**: Working memory model compilation failures

**Definition of Done:**
- Working memory model compiles without field errors
- Hebbian strength calculation works with available data
- No undefined field references remain
- Model handles missing fields gracefully

---

### BMP-HIGH-012: Implement query result caching for expensive operations
**Epic**: Performance Optimization  
**Story Points**: 5
**Priority**: High
**Labels**: `performance`, `caching`, `llm-optimization`

**Description:**
Expensive LLM-enriched queries are re-executed on each dbt run without result caching, causing repeated 500ms+ operations that break biological timing targets.

**Acceptance Criteria:**
- [ ] Implement query result cache with biological rhythm-aware TTL
- [ ] Cache expensive LLM-enriched transformation results
- [ ] Add cache hit/miss monitoring and metrics
- [ ] Implement cache invalidation on source data changes
- [ ] Add cache size limits and LRU eviction
- [ ] Test cache effectiveness across biological processing cycles

**Technical Details:**
- **Files**: Multiple SQL models with LLM operations
- **Current**: Every dbt run re-executes expensive LLM calls
- **Target**: Cache results with appropriate TTL (5min working memory, 1hr consolidation)
- **Performance Impact**: 500ms+ → <50ms for cached results

**Definition of Done:**
- Query result caching implemented across LLM-heavy models
- Cache hit rate >80% for repeated operations within biological cycles
- Biological timing targets achievable with cached results
- Cache monitoring integrated with performance dashboard

---

## Medium Priority Stories

### BMP-MEDIUM-011: Standardize null safety patterns across models
**Epic**: Code Quality
**Story Points**: 8
**Priority**: Medium
**Labels**: `sql`, `null-safety`, `standardization`

**Description:**
Inconsistent use of COALESCE() and NULLIF() with different default values across models creates unpredictable behavior.

**Acceptance Criteria:**
- [ ] Define standard null safety patterns for the project
- [ ] Create utility macros for common null handling scenarios
- [ ] Update all models to use consistent patterns
- [ ] Add validation tests for null safety
- [ ] Document null handling standards
- [ ] Create code review checklist for null safety

**Technical Details:**
- **Files**: Multiple models in `models/` directory
- **Issue**: Mixed null handling approaches, inconsistent defaults
- **Solution**: Standardized macros and patterns

**Definition of Done:**
- All models use consistent null safety patterns
- Utility macros handle common null scenarios
- Test suite validates null handling behavior
- Documentation provides clear null safety guidelines

### BMP-MEDIUM-012: Replace hardcoded parameters with dbt variables
**Epic**: Configuration Management
**Story Points**: 3
**Priority**: Medium
**Labels**: `configuration`, `macros`, `biological-parameters`

**Description:**
Biological parameter validation uses hardcoded values instead of configurable dbt variables.

**Acceptance Criteria:**
- [ ] Replace all hardcoded parameter thresholds with `var()` calls
- [ ] Add parameter definitions to dbt_project.yml
- [ ] Update parameter validation macros
- [ ] Create parameter configuration documentation
- [ ] Add validation for parameter ranges
- [ ] Test parameter changes across environments

**Technical Details:**
- **File**: `macros/biological_memory_macros.sql:23-29`
- **Issue**: Hardcoded values (0.5, 0.01) instead of variables
- **Solution**: Use dbt variables for all biological parameters

**Definition of Done:**
- No hardcoded parameters remain in macros
- All parameters are configurable via dbt_project.yml
- Parameter validation uses configurable thresholds
- Documentation explains parameter configuration

### BMP-MEDIUM-013: Optimize episode clustering performance
**Epic**: Performance Optimization
**Story Points**: 5
**Priority**: Medium
**Labels**: `performance`, `sql-optimization`, `indexing`

**Description:**
Multiple window functions with complex partitioning in episode clustering may cause performance bottlenecks.

**Acceptance Criteria:**
- [ ] Analyze query execution plans for performance bottlenecks
- [ ] Add appropriate indexes for partitioning columns
- [ ] Consider materialization strategies for intermediate results
- [ ] Implement query performance monitoring
- [ ] Add performance tests with realistic data volumes
- [ ] Create performance optimization documentation

**Technical Details:**
- **File**: `models/short_term_memory/stm_hierarchical_episodes.sql:91-117`
- **Risk**: Slow query performance with large data volumes
- **Solution**: Indexing strategies and materialization optimization

**Definition of Done:**
- Episode clustering queries perform within acceptable limits
- Appropriate indexes are created automatically
- Performance monitoring tracks query execution times
- Performance tests validate behavior with large datasets

---

## Low Priority Stories

### BMP-LOW-001: Implement secure environment variable handling
**Epic**: Security Hardening
**Story Points**: 2
**Priority**: Low
**Labels**: `security`, `configuration`, `environment-variables`

**Description:**
Direct use of env_var() in SQL templates could expose configuration details.

**Acceptance Criteria:**
- [ ] Replace env_var() calls with dbt variables
- [ ] Add environment variable validation
- [ ] Implement secure configuration loading
- [ ] Update templates to use variables instead of direct env access
- [ ] Add configuration security documentation

**Technical Details:**
- **File**: `models/short_term_memory/stm_hierarchical_episodes.sql:188`
- **Security Risk**: Minor configuration exposure
- **Solution**: Use dbt variables instead of direct env_var calls

**Definition of Done:**
- No direct env_var() usage in SQL templates
- Configuration is loaded securely through dbt variables
- Environment validation prevents misconfiguration
- Security documentation covers configuration best practices

### BMP-LOW-002: Standardize code formatting and documentation
**Epic**: Developer Experience
**Story Points**: 3
**Priority**: Low
**Labels**: `code-quality`, `documentation`, `developer-experience`

**Description:**
Inconsistent code formatting and comment styles make maintenance more difficult.

**Acceptance Criteria:**
- [ ] Apply Black formatting to all Python files
- [ ] Standardize SQL formatting with sqlfmt
- [ ] Update docstring format to follow conventions
- [ ] Add pre-commit hooks for formatting
- [ ] Create style guide documentation
- [ ] Update existing code to follow standards

**Technical Details:**
- **Files**: Multiple Python and SQL files
- **Issue**: Mixed comment styles, inconsistent formatting
- **Solution**: Automated formatting tools and standards

**Definition of Done:**
- All code follows consistent formatting standards
- Pre-commit hooks enforce formatting automatically
- Style guide documentation is available
- Existing code has been reformatted

---

## Architecture Alignment Stories (Agent-3)

### BMP-ARCH-001: Implement 4-stage biological pipeline orchestration
**Epic**: Architecture Alignment
**Story Points**: 13
**Priority**: Critical
**Labels**: `architecture`, `orchestration`, `biological-rhythms`

**Description:**
Architecture specifies detailed cron-based biological rhythm scheduling but implementation lacks proper staged orchestration.

**Acceptance Criteria:**
- [ ] Implement Working Memory processing every minute (6-22h) 
- [ ] Implement STM processing every 5 minutes
- [ ] Implement Consolidation processing every 30 minutes  
- [ ] Implement LTM processing every 6 hours
- [ ] Add biological rhythm monitoring and health checks
- [ ] Create pipeline orchestration service matching architecture spec
- [ ] Test continuous biological memory processing flow

**Technical Details:**
- **Architecture Reference**: Lines 1423-1463 in ARCHITECTURE.md
- **Current State**: Models exist but no scheduled orchestration
- **Required**: Cron scheduling, pipeline coordination, biological timing

**Definition of Done:**
- Pipeline runs continuously with biological rhythms
- Each stage processes at specified biological intervals
- Health monitoring tracks pipeline biological state
- System runs as continuous biological processor, not batch

### BMP-ARCH-002: Fix STM duration parameter to match biological specification
**Epic**: Biological Parameters
**Story Points**: 2  
**Priority**: Critical
**Labels**: `biological-parameters`, `stm-duration`, `neuroscience`

**Description:**
STM duration set to 30 seconds instead of 30 minutes, violating biological fidelity by 60x.

**Acceptance Criteria:**
- [ ] Change `short_term_memory_duration: 30` from seconds to minutes in dbt_project.yml
- [ ] Update all STM timing logic to use 30 minutes
- [ ] Validate biological memory transfer timing
- [ ] Add parameter validation to prevent future mismatches
- [ ] Update documentation with correct biological parameters
- [ ] Test memory transitions with correct timing

**Technical Details:**
- **Architecture Spec**: `stm_duration_minutes: 30` (30 minutes)
- **Current Bug**: `short_term_memory_duration: 30` (30 seconds)
- **Impact**: Memories transfer 60x faster than biological specification

**Definition of Done:**
- STM duration matches 30-minute biological specification
- Memory transitions occur at biologically accurate intervals
- Parameter validation prevents future mismatches
- Biological fidelity is restored to specification

### BMP-ARCH-003: Implement REM sleep creative association simulation
**Epic**: Biological Processes
**Story Points**: 8
**Priority**: High  
**Labels**: `rem-sleep`, `creative-associations`, `consolidation`

**Description:**
Missing REM sleep simulation for cross-domain creative insights as specified in architecture.

**Acceptance Criteria:**
- [ ] Implement nightly REM sleep simulation process
- [ ] Add creative connection discovery between distant memories
- [ ] Create cross-category semantic association detection
- [ ] Add creative insight storage and retrieval
- [ ] Schedule REM sleep process for nightly execution (3 AM)
- [ ] Add monitoring for creative association discovery rate
- [ ] Test creative connection quality and relevance

**Technical Details:**
- **Architecture Reference**: Lines 1467-1529 (REM sleep simulation)
- **Process**: Random memory sampling, creative LLM connections, insight storage
- **Schedule**: Nightly at 3 AM with 90-minute cycle simulation

**Definition of Done:**
- REM sleep process runs nightly as specified
- Creative connections are discovered between memory domains
- Insights are stored and accessible for retrieval
- Process monitors and reports creative discovery metrics

### BMP-ARCH-004: Implement synaptic homeostasis process
**Epic**: Biological Processes  
**Story Points**: 5
**Priority**: High
**Labels**: `synaptic-homeostasis`, `memory-strength`, `normalization`

**Description:**
Missing weekly synaptic homeostasis to prevent memory strength saturation and runaway potentiation.

**Acceptance Criteria:**
- [ ] Implement weekly memory strength normalization
- [ ] Add global memory strength downscaling (0.9x factor)
- [ ] Implement weak connection pruning (<0.01 strength)
- [ ] Add cortical column rebalancing algorithm
- [ ] Schedule process for Sunday 4 AM as specified
- [ ] Add monitoring for memory strength distribution
- [ ] Prevent memory saturation and maintain biological balance

**Technical Details:**
- **Architecture Reference**: Lines 1531-1564 (synaptic homeostasis)
- **Process**: Global downscaling, weak pruning, column rebalancing
- **Schedule**: Weekly on Sunday at 4 AM

**Definition of Done:**
- Memory strengths are normalized weekly
- Weak connections are pruned automatically
- Cortical columns maintain balanced activation
- System prevents memory strength saturation

### BMP-ARCH-005: Implement PostgreSQL integration for codex-store flow  
**Epic**: Data Integration
**Story Points**: 8
**Priority**: High
**Labels**: `postgresql`, `data-integration`, `schema`

**Description:**
Implementation doesn't match architecture's codex-store → codex-dreams → codex-memory PostgreSQL flow.

**Acceptance Criteria:**
- [ ] Configure DuckDB postgres_scanner extension
- [ ] Implement source table connections to PostgreSQL
- [ ] Create target table writes back to PostgreSQL
- [ ] Match schema specifications in architecture
- [ ] Test full data flow: codex-store → processing → codex-memory
- [ ] Add PostgreSQL connection monitoring
- [ ] Validate data integrity across PostgreSQL integration

**Technical Details:**
- **Architecture Reference**: PostgreSQL schemas lines 224-476
- **Missing**: postgres_scanner configuration, schema matching
- **Required**: Source reads and target writes via PostgreSQL

**Definition of Done:**
- DuckDB connects to PostgreSQL via postgres_scanner
- Source data flows from codex-store PostgreSQL tables
- Processed insights write back to codex-memory tables
- Full integration matches architecture data flow specification

### BMP-ARCH-006: Implement LLM prompt caching optimization
**Epic**: Performance Optimization
**Story Points**: 5
**Priority**: High
**Labels**: `llm`, `caching`, `performance`, `optimization`

**Description:**
Missing comprehensive LLM cache system specified in architecture causing performance degradation.

**Acceptance Criteria:**
- [ ] Implement prompt response caching with hash keys
- [ ] Add cache hit/miss monitoring and metrics
- [ ] Create cache duration configuration (24h default)
- [ ] Add cache cleanup and maintenance process
- [ ] Implement cache warming for common prompts
- [ ] Add cache performance monitoring dashboard
- [ ] Test cache effectiveness and performance improvement

**Technical Details:**
- **Architecture Reference**: Lines 2072-2120 (LLM caching)
- **Implementation**: Hash-based prompt caching with PostgreSQL storage
- **Benefits**: Reduce LLM costs, improve response times

**Definition of Done:**
- LLM responses are cached and reused effectively
- Cache hit rate monitoring shows performance improvement
- Cache maintenance prevents stale data issues
- System reduces LLM API costs through caching

### BMP-ARCH-007: Complete cortical minicolumn architecture implementation
**Epic**: Semantic Networks
**Story Points**: 8
**Priority**: Medium
**Labels**: `cortical-minicolumns`, `semantic-organization`, `neuroscience`

**Description:**
Cortical minicolumn architecture referenced but incompletely implemented.

**Acceptance Criteria:**
- [ ] Implement full 1000 minicolumn semantic organization
- [ ] Create 50 cortical regions with 20 minicolumns each  
- [ ] Add semantic category assignment to minicolumns
- [ ] Implement minicolumn competition and winner-take-all
- [ ] Add cortical region specialization logic
- [ ] Test semantic organization and memory assignment
- [ ] Validate biological cortical organization principles

**Technical Details:**
- **Architecture Reference**: Lines 1095-1225 (cortical minicolumns)
- **Structure**: 1000 minicolumns, 50 regions, semantic specialization
- **Current**: Partial references without full implementation

**Definition of Done:**
- 1000 cortical minicolumns are fully implemented
- Semantic memories are properly assigned to specialized regions
- Minicolumn competition works as specified
- Cortical organization matches biological principles

### BMP-ARCH-008: Configure DuckDB extensions and performance settings
**Epic**: Performance Optimization
**Story Points**: 3
**Priority**: Medium  
**Labels**: `duckdb`, `extensions`, `performance`, `configuration`

**Description:**
Missing DuckDB extensions (httpfs, postgres, json) and performance configuration specified in architecture.

**Acceptance Criteria:**
- [ ] Install and configure httpfs extension for web access
- [ ] Install and configure postgres extension for PostgreSQL integration
- [ ] Install and configure json extension for JSON processing
- [ ] Add memory limit and threading configuration
- [ ] Implement performance monitoring for DuckDB operations
- [ ] Test extension functionality and integration
- [ ] Document extension configuration and usage

**Technical Details:**
- **Architecture Reference**: Extension requirements throughout architecture
- **Missing**: httpfs, postgres, json extensions
- **Required**: Extension installation and configuration

**Definition of Done:**
- All required DuckDB extensions are installed and working
- Performance settings match architecture specification
- Extensions enable required integration capabilities
- Documentation covers extension configuration

### BMP-ARCH-009: Implement biological constraint validation test suite
**Epic**: Quality Assurance
**Story Points**: 5  
**Priority**: Medium
**Labels**: `testing`, `biological-constraints`, `validation`

**Description:**
Missing comprehensive biological fidelity tests to validate system maintains biological accuracy.

**Acceptance Criteria:**
- [ ] Implement Miller's Law capacity constraint tests (7±2 items)
- [ ] Add forgetting curve validation tests
- [ ] Create Hebbian learning pattern validation
- [ ] Add memory age progression tests  
- [ ] Implement consolidation threshold validation
- [ ] Create biological parameter range validation
- [ ] Test interference and competition mechanisms

**Technical Details:**
- **Architecture Reference**: Lines 1884-1925 (biological constraint tests)
- **Missing**: Comprehensive biological accuracy validation
- **Required**: Test suite validating neuroscience principles

**Definition of Done:**
- Test suite validates all biological constraints
- Tests catch violations of neuroscience principles
- Continuous integration runs biological validation
- System maintains biological accuracy automatically

---

## Integration Fix Stories (Agent-4)

### BMP-INTEGRATION-001: Resolve dual PostgreSQL connection mechanism conflicts
**Epic**: Database Integration
**Story Points**: 5
**Priority**: Critical  
**Labels**: `integration`, `database`, `postgresql`, `duckdb`

**Description:**
The system has two conflicting PostgreSQL connection mechanisms that create resource conflicts and connection failures.

**Acceptance Criteria:**
- [ ] Consolidate to single PostgreSQL connection method
- [ ] Remove either profiles.yml attach OR setup SQL secrets/attach approach
- [ ] Test connection reliability under load
- [ ] Ensure no resource leaks or connection conflicts
- [ ] Update documentation with chosen connection pattern

**Technical Details:**
- **Files**: `profiles.yml:18-20`, `setup_postgres_connection.sql:16-26`
- **Conflict**: dbt profiles.yml `attach` vs SQL `CREATE SECRET` + `ATTACH`
- **Impact**: Connection failures, resource leaks, undefined behavior

**Definition of Done:**
- Single, consistent PostgreSQL connection mechanism
- All dbt models connect reliably to PostgreSQL
- No connection resource conflicts
- Integration tests pass with stable connections

---

### BMP-INTEGRATION-002: Standardize Ollama endpoint and model configurations
**Epic**: LLM Integration
**Story Points**: 3
**Priority**: High
**Labels**: `integration`, `llm`, `configuration`, `ollama`

**Description:**
Ollama endpoint URLs and model names are inconsistent across components, causing LLM service failures.

**Acceptance Criteria:**
- [ ] Standardize Ollama URL configuration to single environment variable
- [ ] Align model names across all service components
- [ ] Update orchestrator to use environment variables consistently
- [ ] Test LLM service initialization and UDF registration
- [ ] Verify biological memory processing with consistent LLM configuration

**Technical Details:**
- **Files**: `.env:41`, `orchestrate_biological_memory.py:127`, `llm_integration_service.py:57`
- **Issue**: `localhost:11434` vs `192.168.1.110:11434`, `qwen2.5:0.5b` vs `gpt-oss:20b`
- **Impact**: LLM initialization failures, missing UDF functions

**Definition of Done:**
- All components use consistent Ollama configuration
- LLM service initializes successfully
- UDF functions register without errors
- Biological memory processing works with LLM integration

---

### BMP-INTEGRATION-003: Fix database path configuration conflicts  
**Epic**: Infrastructure Configuration
**Story Points**: 3
**Priority**: High
**Labels**: `integration`, `configuration`, `database-paths`

**Description:**
Database paths are hardcoded in orchestrator but configurable in profiles, causing deployment inconsistencies.

**Acceptance Criteria:**
- [ ] Make orchestrator respect DUCKDB_PATH environment variable
- [ ] Ensure consistent database paths between orchestrator and dbt profiles
- [ ] Test deployment with different database path configurations
- [ ] Update error handling for missing database paths
- [ ] Validate integration tests with custom paths

**Technical Details:**
- **File**: `orchestrate_biological_memory.py:138`
- **Issue**: `self.base_path / 'dbs' / 'memory.duckdb'` ignores `DUCKDB_PATH`
- **Impact**: Production deployment failures, path inconsistencies

**Definition of Done:**
- Orchestrator uses configurable database paths
- Consistent paths between all components
- Deployment works across environments
- Integration tests validate path configuration

---

### BMP-INTEGRATION-004: Add error handling to dbt source references
**Epic**: Data Integration
**Story Points**: 5
**Priority**: High
**Labels**: `integration`, `dbt`, `error-handling`, `data-pipeline`

**Description:**
dbt models assume PostgreSQL source is always available with no graceful fallbacks, causing complete pipeline failures.

**Acceptance Criteria:**
- [ ] Add conditional logic for source availability in dbt models
- [ ] Implement graceful fallbacks when PostgreSQL connection fails
- [ ] Add error logging and alerting for source failures
- [ ] Test pipeline behavior with unavailable PostgreSQL source
- [ ] Ensure working memory and STM processing continue with cached/fallback data

**Technical Details:**
- **Files**: `stg_codex_memories.sql:20`, `wm_active_context.sql:28`, Multiple models
- **Issue**: `{{ source('codex_db', 'memories') }}` has no error handling
- **Impact**: Complete pipeline failure if source database unavailable

**Definition of Done:**
- Pipeline continues operating with PostgreSQL source failures
- Graceful degradation with appropriate error messages
- Fallback mechanisms for critical memory processing
- Monitoring alerts for source connection issues

---

### BMP-INTEGRATION-005: Implement cron integration for biological rhythm scheduling
**Epic**: Orchestration Integration
**Story Points**: 5
**Priority**: Medium
**Labels**: `integration`, `orchestration`, `cron`, `scheduling`

**Description:**
Orchestrator implements biological rhythm scheduling but lacks actual cron integration for automated execution.

**Acceptance Criteria:**
- [ ] Create cron configuration files for biological memory cycles
- [ ] Integrate cron schedules with service configuration system
- [ ] Test automated memory consolidation cycles (hourly, daily, weekly)
- [ ] Add monitoring and alerting for missed scheduled cycles
- [ ] Document biological rhythm scheduling setup

**Technical Details:**
- **Files**: `orchestrate_biological_memory.py:588-620`, `service_configs/` directory
- **Issue**: Scheduler code exists but no cron files or system integration
- **Impact**: Memory consolidation cycles don't run automatically

**Definition of Done:**
- Automated biological rhythm scheduling works
- Memory consolidation runs on schedule
- System service integration complete
- Monitoring validates scheduled execution

---

### BMP-INTEGRATION-006: Fix test environment isolation
**Epic**: Test Infrastructure
**Story Points**: 3
**Priority**: Medium
**Labels**: `integration`, `testing`, `environment-isolation`

**Description:**
Tests use database configurations inconsistently, risking production data contamination and unreliable CI/CD.

**Acceptance Criteria:**
- [ ] Standardize all tests to use TEST_DATABASE_URL exclusively
- [ ] Remove production database references from test files
- [ ] Add test database isolation validation
- [ ] Update CI/CD configuration for proper test isolation
- [ ] Verify no tests can access production data

**Technical Details:**
- **Files**: `conftest.py:19`, `environment_test.py:47`, Multiple test files
- **Issue**: Tests use `POSTGRES_DB_URL` instead of `TEST_DATABASE_URL`
- **Impact**: Test failures, production data contamination risk

**Definition of Done:**
- All tests use isolated test database
- No production database access from tests
- CI/CD pipeline validates test isolation
- Test database cleanup works properly

---

### BMP-INTEGRATION-007: Standardize environment variable naming
**Epic**: Configuration Management
**Story Points**: 2
**Priority**: Medium
**Labels**: `integration`, `configuration`, `environment-variables`

**Description:**
Multiple environment variables serve the same purpose, creating configuration confusion and integration issues.

**Acceptance Criteria:**
- [ ] Consolidate DATABASE_URL, DB_CONN, POSTGRES_DB_URL to single variable
- [ ] Update all components to use standardized variable names
- [ ] Add backward compatibility during transition period
- [ ] Update documentation with new environment variable standards
- [ ] Test all integrations with new variable naming

**Technical Details:**
- **Files**: `.env:6-11`, Multiple configuration files
- **Issue**: 3 different database URL variables with same values
- **Impact**: Services may connect to wrong databases

**Definition of Done:**
- Single, clear naming convention for environment variables
- All components use standardized variable names
- Documentation reflects new standards
- No configuration ambiguity

---

**Total Stories Created**: 26
**Story Points**: 100
**Estimated Effort**: 12-14 sprints for complete resolution

**Priority Distribution**:
- Critical: 4 stories (23 points)  
- High: 12 stories (60 points)
- Medium: 9 stories (32 points)
- Low: 1 story (5 points)

**Integration Priority**:
1. **CRITICAL**: Database connection conflicts and path configurations (8 points)
2. **HIGH**: LLM integration and data pipeline error handling (16 points)  
3. **MEDIUM**: Cron scheduling, test isolation, and configuration standardization (10 points)

---

## Test Quality Stories (Agent-2)

### BMP-TEST-001: Fix test database isolation and hardcoded paths
**Epic**: Test Infrastructure  
**Story Points**: 5
**Priority**: Critical
**Labels**: `testing`, `test-isolation`, `database`

**Description:**
Test fixtures use hardcoded production database paths, breaking test isolation and potentially corrupting production data.

**Acceptance Criteria:**
- [ ] Replace hardcoded database path in `test_working_memory.py:28` 
- [ ] Use `test_duckdb` fixture from conftest.py for all test database connections
- [ ] Ensure all tests use isolated test database instances
- [ ] Add test database cleanup after test runs
- [ ] Verify tests cannot access production database
- [ ] Add environment variable validation for test database URLs

**Technical Details:**
- **File**: `tests/memory/test_working_memory.py:28`
- **Current**: `conn = duckdb.connect('/Users/ladvien/biological_memory/dbs/memory.duckdb')`
- **Expected**: Use `test_duckdb` fixture for proper isolation
- **Risk**: Production database corruption, non-isolated test runs

**Definition of Done:**
- All tests use isolated test database fixtures
- No tests can access production database
- Test database isolation is verified and enforced
- Documentation explains proper test database usage

---

### BMP-TEST-002: Consolidate duplicate test directories 
**Epic**: Test Architecture
**Story Points**: 8
**Priority**: Critical
**Labels**: `testing`, `architecture`, `consolidation`

**Description:**
Tests exist in both `/tests/` and `/biological_memory/tests/` directories, creating maintenance burden and inconsistent results.

**Acceptance Criteria:**
- [ ] Audit both test directories for overlapping coverage
- [ ] Choose primary test location (recommend `/tests/`)
- [ ] Migrate or consolidate duplicate tests
- [ ] Remove duplicate test files and infrastructure
- [ ] Update CI/CD pipeline to run unified test suite
- [ ] Update documentation with single test location
- [ ] Verify no regression in test coverage

**Technical Details:**
- **Directories**: `/tests/` (53 files) and `/biological_memory/tests/` (62+ files)
- **Issue**: Duplicate test coverage, conflicting results, doubled CI time
- **Solution**: Single consolidated test directory structure

**Definition of Done:**
- Single test directory structure in place
- No duplicate test files exist
- Test coverage maintained or improved
- CI/CD runs unified test suite
- Documentation reflects new structure

---

### BMP-TEST-003: Add comprehensive biological parameter testing
**Epic**: Biological Accuracy Testing
**Story Points**: 8  
**Priority**: High
**Labels**: `testing`, `biological-parameters`, `validation`

**Description:**
Missing test coverage for critical biological parameters from ARCHITECTURE.md including STM duration, forgetting rates, and Hebbian learning.

**Acceptance Criteria:**
- [ ] Add tests for `stm_duration_minutes: 30` timeout behavior
- [ ] Add tests for `forgetting_rate: 0.95` curve calculations
- [ ] Add tests for `hebbian_learning_rate: 0.1` strengthening
- [ ] Add tests for `consolidation_threshold: 0.5` enforcement
- [ ] Add tests for `working_memory_capacity: 7` constraint enforcement
- [ ] Add tests for biological timing constraints (5-minute windows)
- [ ] Create biological parameter validation test suite

**Technical Details:**
- **Missing Coverage**: Core biological parameters from architecture
- **Impact**: Biological accuracy not validated by tests
- **Solution**: Comprehensive parameter validation test suite

**Definition of Done:**
- All biological parameters have corresponding tests
- Tests validate parameter constraint enforcement
- Tests catch violations of biological accuracy
- Parameter changes trigger test validation

---

### BMP-TEST-004: Fix mock accuracy for biological parameters
**Epic**: Test Quality
**Story Points**: 5
**Priority**: High
**Labels**: `testing`, `mocks`, `accuracy`

**Description:**
Ollama mock responses use hardcoded values that may not reflect actual biological parameter distributions.

**Acceptance Criteria:**
- [ ] Generate mock responses based on actual biological parameter ranges
- [ ] Use dbt_project.yml parameters to drive mock value generation
- [ ] Add realistic variance to mock responses (not always 0.8, 0.92)
- [ ] Implement deterministic but varied mock responses
- [ ] Add mock validation against real LLM response patterns
- [ ] Update test expectations to handle realistic value ranges

**Technical Details:**
- **File**: `tests/conftest.py:100-150`
- **Issue**: Hardcoded mock values (importance: 0.8, confidence: 0.92)
- **Solution**: Parameter-driven realistic mock generation

**Definition of Done:**
- Mock responses reflect realistic biological parameter ranges
- Mock values are generated from actual parameter distributions
- Tests work with both mocks and real LLM responses
- Mock accuracy is validated against real responses

---

### BMP-TEST-005: Add LLM resilience and timeout testing
**Epic**: LLM Integration Testing
**Story Points**: 5
**Priority**: High
**Labels**: `testing`, `llm`, `resilience`, `timeout`

**Description:**
Missing tests for LLM failure scenarios including timeouts, malformed responses, and server unavailability.

**Acceptance Criteria:**
- [ ] Add tests for Ollama server timeout scenarios
- [ ] Add tests for malformed LLM response parsing
- [ ] Add tests for LLM rate limiting handling
- [ ] Add tests for Ollama server unavailability
- [ ] Add tests for circuit breaker activation
- [ ] Add tests for LLM operation retry logic
- [ ] Add tests for fallback mechanisms

**Technical Details:**
- **Missing Coverage**: LLM error and timeout scenarios
- **Risk**: Production failures not caught by test suite
- **Solution**: Comprehensive LLM resilience testing

**Definition of Done:**
- LLM failure scenarios are tested comprehensively
- Timeout behavior is validated with test scenarios
- Circuit breaker patterns are tested
- Fallback mechanisms are validated

---

### BMP-TEST-006: Add performance and load testing for biological constraints
**Epic**: Performance Testing
**Story Points**: 8
**Priority**: High
**Labels**: `testing`, `performance`, `load`, `biological-constraints`

**Description:**
No load or stress testing exists to validate biological timing constraints under realistic memory loads.

**Acceptance Criteria:**
- [ ] Add tests for working memory under high load (>1000 raw memories)
- [ ] Add tests for STM processing with concurrent episodes
- [ ] Add tests for consolidation performance with large semantic networks
- [ ] Add tests validating biological timing targets (<100ms working memory)
- [ ] Add stress tests for capacity limits (Miller's 7±2)
- [ ] Add performance regression tests
- [ ] Add memory leak detection in long-running tests

**Technical Details:**
- **Missing Coverage**: Performance testing with biological constraints
- **Risk**: System may not handle realistic memory loads
- **Solution**: Comprehensive performance and load test suite

**Definition of Done:**
- Performance tests validate biological timing constraints
- Load tests verify system handles realistic memory volumes
- Stress tests validate capacity limit enforcement
- Performance regression detection is automated

---

### BMP-TEST-007: Add schema validation and broken reference testing
**Epic**: Schema Testing
**Story Points**: 3
**Priority**: Medium
**Labels**: `testing`, `schema`, `validation`

**Description:**
No tests validate table existence or catch broken references before deployment (validates Agent-1 findings).

**Acceptance Criteria:**
- [ ] Add tests to validate all referenced tables exist
- [ ] Add tests to validate column existence in referenced tables
- [ ] Add schema migration testing
- [ ] Add tests for foreign key constraint validation
- [ ] Add tests for missing table scenarios
- [ ] Add integration with dbt model compilation testing

**Technical Details:**
- **Cross-Reference**: Agent-1 found missing `semantic_associations` and `network_centrality` tables
- **Gap**: Schema validation missing from test suite
- **Solution**: Automated schema validation testing

**Definition of Done:**
- Schema validation tests catch missing table references
- Tests validate all model dependencies exist
- Broken references are caught before deployment
- Schema changes trigger validation testing

---

### BMP-TEST-008: Fix timestamp sensitivity and flaky tests
**Epic**: Test Reliability
**Story Points**: 3
**Priority**: Medium
**Labels**: `testing`, `reliability`, `timestamps`

**Description:**
Tests using `datetime.now()` with minute/second calculations create timing-dependent failures.

**Acceptance Criteria:**
- [ ] Replace `datetime.now()` usage in tests with fixed timestamps
- [ ] Add time mocking for timestamp-sensitive tests
- [ ] Implement test time control utilities
- [ ] Add buffer tolerances for timing-sensitive assertions
- [ ] Identify and fix all timestamp-dependent test logic
- [ ] Add test stability monitoring

**Technical Details:**
- **Files**: `tests/memory/test_working_memory.py:194-227`, `integration_pipeline_test.py`
- **Issue**: Timing edge cases cause intermittent test failures
- **Solution**: Fixed timestamps and time mocking

**Definition of Done:**
- Tests use fixed, controlled timestamps
- No timing-dependent test failures occur
- Time mocking utilities are available for timestamp tests
- Test stability is improved and monitored

---

### BMP-TEST-009: Improve test data realism and edge cases
**Epic**: Test Quality
**Story Points**: 5
**Priority**: Medium
**Labels**: `testing`, `test-data`, `edge-cases`

**Description:**
Test fixtures use artificially clean data unlike real-world memory patterns, potentially missing edge cases.

**Acceptance Criteria:**
- [ ] Add realistic test data with messy, unstructured content
- [ ] Add test cases with very long text content
- [ ] Add test cases with special characters and unicode
- [ ] Add test cases with mixed languages
- [ ] Add test cases with malformed or incomplete data
- [ ] Add edge case scenarios (empty, null, whitespace-only)
- [ ] Add test data generation utilities for realistic patterns

**Technical Details:**
- **Files**: `tests/conftest.py`, `test_working_memory.py:52-136`
- **Issue**: Artificially clean test data doesn't reflect real usage
- **Solution**: Realistic test data with comprehensive edge cases

**Definition of Done:**
- Test data includes realistic messy content patterns
- Edge cases are thoroughly covered by test scenarios
- Test data generation utilities create realistic patterns
- Tests validate robustness with real-world data

---

**Updated Total Stories Created**: 28
**Updated Story Points**: 154  
**Updated Estimated Effort**: 16-20 sprints for complete resolution

**Final Priority Distribution**:
- Critical: 5 stories (26 points)
- High: 14 stories (88 points)  
- Medium: 8 stories (35 points)
- Low: 1 story (5 points)

**Agent-2 Test Quality Summary**:
- 9 new test-focused stories added
- 55 additional story points for test improvements  
- Critical test infrastructure issues identified
- Biological accuracy testing gaps addressed
- Cross-validation with Agent-1 findings completed

---

## Security Hardening Stories (Agent-6)

### BMP-SECURITY-001: EMERGENCY - Rotate all exposed credentials
**Epic**: Security Incident Response
**Story Points**: 2
**Priority**: Critical
**Labels**: `security`, `credentials`, `emergency`, `immediate-action`

**Description:**
Production database credentials and JWT secrets are exposed in multiple files. This is an active security incident requiring immediate action.

**Acceptance Criteria:**
- [ ] Rotate production database password "MZSfXiLr5uR3QYbRwv2vTzi22SvFkj4a" immediately
- [ ] Rotate test database password "TestPass_2024_Secure" immediately  
- [ ] Rotate JWT secret "U7wfix5LN6HPXAQX2RUxeuI7SN1k4kCRb75JRKPW1oM=" immediately
- [ ] Update all services with new credentials
- [ ] Verify no unauthorized access occurred
- [ ] Document credential rotation process

**Technical Details:**
- **Files**: `.env:6-10,14,20,24-27,60`, `biological_memory/setup_postgres_connection.sql:22`
- **Risk Level**: CRITICAL - Complete system compromise possible
- **Impact**: Database breach, authentication bypass, data theft

**Definition of Done:**
- All exposed credentials have been rotated
- New credentials are not exposed in code or logs
- Services function with new credentials
- No evidence of unauthorized access

---

### BMP-SECURITY-002: Implement secure credential management system
**Epic**: Credential Management
**Story Points**: 8
**Priority**: Critical
**Labels**: `security`, `credential-management`, `infrastructure`

**Description:**
Replace exposed credentials with proper credential management system using HashiCorp Vault, AWS Secrets Manager, or similar.

**Acceptance Criteria:**
- [ ] Choose and deploy credential management system (HashiCorp Vault recommended)
- [ ] Migrate all database credentials to secure storage
- [ ] Migrate all API keys and secrets to secure storage
- [ ] Implement credential rotation capabilities
- [ ] Update all services to retrieve credentials from secure store
- [ ] Add credential access audit logging
- [ ] Remove all hardcoded credentials from code

**Technical Details:**
- **Current State**: Credentials in plain text files
- **Target**: Secure credential store with rotation
- **Integration**: All services must authenticate to credential store

**Definition of Done:**
- No credentials exist in plain text anywhere
- Credential store is properly secured and monitored
- Services retrieve credentials dynamically
- Credential rotation is automated
- Audit logging tracks credential access

---

### BMP-SECURITY-003: Enable authentication and access controls
**Epic**: Authentication Framework
**Story Points**: 5
**Priority**: High
**Labels**: `security`, `authentication`, `access-control`

**Description:**
Authentication is disabled (`MCP_AUTH_ENABLED=false`) leaving API endpoints unprotected.

**Acceptance Criteria:**
- [ ] Enable MCP authentication (`MCP_AUTH_ENABLED=true`)
- [ ] Implement proper JWT token validation
- [ ] Add API endpoint authentication requirements
- [ ] Implement role-based access control (RBAC)
- [ ] Add authentication to all LLM and database endpoints
- [ ] Enable circuit breaker for security (`MCP_CIRCUIT_BREAKER_ENABLED=true`)
- [ ] Add authentication monitoring and alerting

**Technical Details:**
- **Files**: `.env:58,61`
- **Current**: Authentication explicitly disabled
- **Required**: Full authentication and authorization framework

**Definition of Done:**
- All API endpoints require authentication
- JWT validation works properly
- RBAC controls access to sensitive operations
- Authentication failures are monitored and alerted
- Circuit breaker protects against attacks

---

### BMP-SECURITY-004: Implement input validation and prompt sanitization
**Epic**: Input Validation
**Story Points**: 8
**Priority**: High
**Labels**: `security`, `input-validation`, `prompt-injection`, `llm-security`

**Description:**
User prompts are passed directly to LLM without sanitization, creating prompt injection vulnerabilities.

**Acceptance Criteria:**
- [ ] Implement comprehensive input validation framework
- [ ] Add prompt sanitization for LLM inputs
- [ ] Add SQL injection prevention for database inputs
- [ ] Implement content filtering for harmful prompts
- [ ] Add input length and complexity limits
- [ ] Add malicious pattern detection and blocking
- [ ] Add validation monitoring and alerting
- [ ] Create security testing for injection attempts

**Technical Details:**
- **File**: `biological_memory/llm_integration_service.py:193-266`
- **Vulnerability**: Direct prompt passing without validation
- **Risk**: Prompt injection, LLM jailbreaking, information disclosure

**Definition of Done:**
- All user inputs are validated and sanitized
- Prompt injection attacks are blocked
- SQL injection is prevented
- Malicious content is detected and blocked
- Security testing validates protection

---

### BMP-SECURITY-005: Implement API authentication for LLM endpoints
**Epic**: API Security
**Story Points**: 5
**Priority**: Medium
**Labels**: `security`, `api-security`, `llm-endpoints`

**Description:**
Ollama LLM endpoints are accessed without authentication, allowing unauthorized usage and potential abuse.

**Acceptance Criteria:**
- [ ] Add authentication headers to Ollama API requests
- [ ] Implement API key management for LLM services
- [ ] Add rate limiting to prevent API abuse
- [ ] Implement network-level access controls
- [ ] Add API usage monitoring and alerting
- [ ] Add connection encryption (TLS/SSL)
- [ ] Implement API quota management

**Technical Details:**
- **Files**: `llm_integration_service.py:209-215`, SQL models with LLM calls
- **Issue**: Plain HTTP without authentication
- **Risk**: Unauthorized LLM usage, API abuse, resource exhaustion

**Definition of Done:**
- LLM API requests include proper authentication
- Rate limiting prevents abuse
- Network access is controlled and monitored
- API usage is tracked and limited
- Connections are encrypted

---

### BMP-SECURITY-006: Implement security monitoring and audit logging
**Epic**: Security Monitoring
**Story Points**: 8
**Priority**: Medium  
**Labels**: `security`, `monitoring`, `audit-logging`, `incident-response`

**Description:**
No security monitoring, audit logging, or intrusion detection exists to identify security incidents.

**Acceptance Criteria:**
- [ ] Implement comprehensive audit logging for all security events
- [ ] Add authentication failure monitoring and alerting
- [ ] Add suspicious activity detection (unusual API usage, etc.)
- [ ] Implement security log aggregation and analysis
- [ ] Add intrusion detection for database access
- [ ] Create security incident response procedures
- [ ] Add security dashboard for monitoring
- [ ] Implement automated security alerting

**Technical Details:**
- **Missing**: Security monitoring infrastructure
- **Required**: Audit logs, intrusion detection, incident response
- **Integration**: Security logs, monitoring dashboard, alert system

**Definition of Done:**
- All security events are logged and monitored
- Suspicious activity triggers automated alerts
- Security logs are aggregated and analyzable
- Incident response procedures are documented
- Security dashboard provides real-time visibility

---

### BMP-SECURITY-007: Reduce database connection limits and add monitoring
**Epic**: Resource Security
**Story Points**: 3
**Priority**: Medium
**Labels**: `security`, `resource-management`, `monitoring`

**Description:**
Excessive database connection limit (160) creates resource exhaustion attack surface.

**Acceptance Criteria:**
- [ ] Reduce MAX_DB_CONNECTIONS to reasonable limit (20-50)
- [ ] Add connection pool monitoring and alerting
- [ ] Implement connection usage analytics
- [ ] Add protection against connection exhaustion attacks
- [ ] Add circuit breaker for connection pool exhaustion
- [ ] Monitor connection patterns for anomalies
- [ ] Add connection rate limiting per client

**Technical Details:**
- **File**: `.env:67`
- **Issue**: MAX_DB_CONNECTIONS=160 excessive for single-user system
- **Risk**: Resource exhaustion attacks, connection pool depletion

**Definition of Done:**
- Database connections limited to appropriate levels
- Connection usage is monitored and alerted
- Protection exists against connection exhaustion attacks
- Anomalous connection patterns are detected

---

### BMP-SECURITY-008: Implement error message sanitization
**Epic**: Information Security
**Story Points**: 3
**Priority**: Medium
**Labels**: `security`, `information-disclosure`, `error-handling`

**Description:**
Error messages may expose internal system details and configuration information.

**Acceptance Criteria:**
- [ ] Implement error message sanitization for external exposure
- [ ] Remove internal paths and configuration from error messages
- [ ] Add generic error responses for security-sensitive operations
- [ ] Implement detailed logging for internal debugging (not exposed)
- [ ] Add error message security review process
- [ ] Create secure error handling guidelines

**Technical Details:**
- **File**: `biological_memory/llm_integration_service.py:246-266`
- **Issue**: Exception details exposed without sanitization
- **Risk**: Information disclosure, system fingerprinting

**Definition of Done:**
- Error messages don't expose sensitive information
- Internal debugging information is logged securely
- Security-sensitive errors use generic responses
- Error handling follows security guidelines

---

### BMP-SECURITY-009: Fix production logging and configuration exposure
**Epic**: Configuration Security  
**Story Points**: 2
**Priority**: Low
**Labels**: `security`, `logging`, `configuration`

**Description:**
Debug logging enabled in production may log sensitive information.

**Acceptance Criteria:**
- [ ] Change LOG_LEVEL from debug to INFO or WARN for production
- [ ] Add environment-specific logging configuration
- [ ] Implement log sanitization to prevent credential leakage
- [ ] Add secure logging guidelines
- [ ] Replace direct env_var() calls in SQL with dbt variables
- [ ] Add configuration security validation

**Technical Details:**
- **Files**: `.env:55`, Multiple SQL models with env_var() calls  
- **Issue**: Debug logging and direct environment variable exposure
- **Risk**: Credential leakage, configuration disclosure

**Definition of Done:**
- Production logging levels are appropriate
- Logs don't contain sensitive information
- Configuration access is controlled through dbt variables
- Configuration security is validated

---

### BMP-SECURITY-010: Implement network security and encryption
**Epic**: Network Security
**Story Points**: 5
**Priority**: Medium
**Labels**: `security`, `encryption`, `network-security`

**Description:**
All network communications use unencrypted HTTP, exposing data in transit.

**Acceptance Criteria:**
- [ ] Implement TLS/SSL for all HTTP communications
- [ ] Add certificate management for secure connections
- [ ] Enable database connection encryption
- [ ] Add network segmentation and firewall rules
- [ ] Implement secure communication protocols
- [ ] Add network security monitoring
- [ ] Create network security documentation

**Technical Details:**
- **Current**: HTTP connections without encryption
- **Required**: TLS/SSL encryption, network security controls
- **Impact**: Data transmission security

**Definition of Done:**
- All network communications are encrypted
- Certificates are properly managed
- Network access is segmented and controlled
- Network security is monitored
- Documentation covers network security setup

---

**Final Total Stories Created**: 38
**Final Story Points**: 195
**Final Estimated Effort**: 20-24 sprints for complete resolution

**Final Priority Distribution**:
- Critical: 7 stories (34 points) - IMMEDIATE ACTION REQUIRED
- High: 16 stories (94 points)  
- Medium: 14 stories (62 points)
- Low: 1 story (5 points)

**Security Priority Summary**:
- **EMERGENCY**: Credential rotation must happen immediately (BMP-SECURITY-001)
- **CRITICAL**: Credential management system implementation (BMP-SECURITY-002)  
- **HIGH**: Authentication, input validation, and API security (BMP-SECURITY-003, 004, 005)
- **MEDIUM**: Monitoring, resource limits, and network security (BMP-SECURITY-006, 007, 010)

**Agent-6 Security Analysis Summary**:
- 10 new security-focused stories added
- 41 additional story points for security hardening
- CRITICAL security vulnerabilities identified requiring immediate action
- Comprehensive security architecture recommendations
- Cross-validation with all other agent findings for security impact

---

## Documentation and Clarification Stories

### BMP-DOC-001: Clarify MCP Environment Variable Naming Convention
**Epic**: Documentation and Architecture Clarity
**Story Points**: 1
**Priority**: Low
**Labels**: `documentation`, `naming-convention`, `architecture-clarity`

**Description:**
Environment variables use "MCP_" prefix (e.g., `MCP_CIRCUIT_BREAKER_ENABLED`, `MCP_AUTH_ENABLED`) but this system does not implement Model Context Protocol (MCP). This creates architectural confusion.

**Analysis Results from rust-mcp-developer agent:**
- ✅ NO MCP implementation found in codebase
- ✅ NO MCP server/client code exists  
- ✅ NO MCP protocol handling
- ✅ System is Python/dbt/DuckDB-based biological memory pipeline
- ✅ "MCP" references are generic configuration variables only

**Acceptance Criteria:**
- [ ] Rename `MCP_CIRCUIT_BREAKER_ENABLED` to `CIRCUIT_BREAKER_ENABLED` 
- [ ] Rename `MCP_AUTH_ENABLED` to `AUTH_ENABLED`
- [ ] Update all environment variable documentation
- [ ] Clarify in ARCHITECTURE.md that this is not an MCP system
- [ ] Update security stories to reflect correct variable names

**Technical Details:**
- **Files**: `.env.example:36`, `orchestrate_biological_memory.py:59`
- **Current**: Misleading MCP prefix on generic settings
- **Expected**: Clear, technology-appropriate naming convention

**Definition of Done:**
- Environment variables have accurate naming convention
- Documentation clearly states this is not an MCP implementation
- No confusion remains about system architecture
- All references updated consistently across codebase

---

### BMP-ARCH-001: Document Rust/MCP Integration Opportunities (Future)
**Epic**: Future Architecture Planning  
**Story Points**: 2
**Priority**: Low
**Labels**: `architecture`, `future-planning`, `rust`, `mcp`, `integration-opportunities`

**Description:**
While current system has no Rust or MCP implementations, document potential integration points for future enhancement.

**Current Architecture Analysis:**
- Python/dbt/DuckDB biological memory consolidation system
- PostgreSQL for persistent storage
- Ollama for local LLM processing
- Sophisticated biological accuracy with neuroscience-based parameters

**Potential Future Integrations:**
1. **Rust Performance Layer**: High-performance memory processing components
2. **MCP Server**: Expose biological memory insights via MCP protocol for Claude integration
3. **Rust DuckDB Extensions**: Custom analytical functions for biological computations

**Acceptance Criteria:**
- [ ] Document current architecture accurately (non-Rust, non-MCP)
- [ ] Identify performance bottlenecks suitable for Rust optimization
- [ ] Design MCP server specification for memory insight exposure
- [ ] Create integration roadmap with effort estimates
- [ ] Document API boundaries for future Rust components

**Definition of Done:**
- Clear architectural documentation exists
- Future integration paths are well-defined
- No immediate implementation required
- Roadmap available for future development