# dbt Project Configuration for Biological Memory Pipeline
# This project models biological memory processes using dbt transformations

name: 'biological_memory'
version: '1.0.0'
config-version: 2

# This setting configures which "profile" dbt uses for this project.
profile: 'biological_memory'

# These configurations specify where dbt should look for different types of files.
model-paths: ["models"]
analysis-paths: ["analysis"]
test-paths: ["tests"]
seed-paths: ["data"]
macro-paths: ["macros"]
snapshot-paths: ["snapshots"]

clean-targets:
  - "target"
  - "dbt_packages"
  - "tmp"

# Biological Memory Processing Variables
vars:
  # Core biological parameters
  working_memory_capacity: 7  # Miller's Law: 7Â±2 items
  short_term_memory_duration: 30  # seconds
  long_term_memory_threshold: 0.7  # activation strength threshold
  
  # Memory strength and quality thresholds
  strong_connection_threshold: 0.7  # Strong memory connections
  medium_quality_threshold: 0.6    # Medium quality memories
  high_quality_threshold: 0.8      # High quality memories  
  consolidation_threshold: 0.6     # Minimum for consolidation
  stability_threshold: 0.5         # Basic stability requirement
  overload_threshold: 0.9          # Working memory overload
  
  # Memory decay and strengthening factors
  weak_memory_decay_factor: 0.8    # Decay rate for weak memories
  strong_memory_boost_factor: 1.2  # Boost rate for strong memories
  gradual_forgetting_rate: 0.9     # Rate of gradual memory loss
  default_decay_factor: 0.7        # Standard decay rate
  
  # Creative and semantic association parameters
  plausibility_threshold: 0.6      # Minimum plausibility for associations
  novelty_score_threshold: 0.5     # Minimum novelty for creative associations
  creativity_temperature: 0.7      # LLM temperature for creative tasks
  
  # Temporal windows for memory processing (in hours)
  recent_activity_window: 24       # Recent activity classification
  short_processing_window: 1       # Short-term processing window
  weekly_memory_window: 168        # 7 days in hours
  monthly_memory_window: 720       # 30 days in hours
  memory_cleanup_window: 24        # Hours before cleanup
  
  # Hebbian learning parameters  
  hebbian_learning_rate: 0.1       # Enhanced learning rate for BMP-009
  synaptic_decay_rate: 0.001
  homeostasis_target: 0.5
  plasticity_threshold: 0.6
  
  # Synaptic homeostasis parameters
  homeostasis_adjustment_rate: 0.05
  weak_connection_threshold: 0.01  # Prune connections below this threshold
  
  # REM-sleep creative association parameters
  rem_association_batch_size: 100
  rem_creativity_factor: 0.8
  creative_connection_threshold: 0.4
  association_strengthening_rate: 0.15
  
  # Consolidation parameters
  consolidation_batch_size: 1000
  consolidation_window_hours: 24
  semantic_association_threshold: 0.8
  
  # Performance optimization
  incremental_batch_size: 10000
  max_memory_threads: 4
  temp_materialization_threshold: 100000

# Model configurations - Optimized for <50ms query performance
models:
  biological_memory:
    # Working memory models (ephemeral for speed - most accessed, temporary data)
    working_memory:
      +materialized: ephemeral
      +pre-hook: [
        "{{ log('Processing working memory batch', info=true) }}",
        "SET threads = 4",
        "SET enable_hash_join = true"
      ]
      +post-hook: "{{ calculate_memory_stats('working_memory') }}"
      +tags: ["performance_critical", "real_time"]
      
    # Short-term memory (incremental for real-time updates with performance optimization)
    short_term_memory:
      +materialized: incremental
      +unique_key: memory_id
      +on_schema_change: append_new_columns
      +incremental_strategy: merge
      +pre-hook: [
        "{{ synaptic_homeostasis() }}",
        "SET memory_limit = '4GB'",
        "SET enable_parallel_aggregation = true"
      ]
      +post-hook: [
        "CREATE INDEX IF NOT EXISTS idx_{{ this.name }}_memory_id ON {{ this }} (memory_id)",
        "CREATE INDEX IF NOT EXISTS idx_{{ this.name }}_created_at ON {{ this }} (created_at DESC)",
        "ANALYZE {{ this }}"
      ]
      +tags: ["performance_optimized", "incremental"]
      
    # Long-term memory (tables with comprehensive indexing for complex queries)
    long_term_memory:
      +materialized: table
      +pre-hook: "SET memory_limit = '6GB'"
      +post-hook: [
        "{{ create_memory_indexes() }}",
        "CREATE INDEX IF NOT EXISTS idx_{{ this.name }}_semantic_search ON {{ this }} USING gin(concepts)",
        "CREATE INDEX IF NOT EXISTS idx_{{ this.name }}_stability ON {{ this }} (stability_score DESC, created_at DESC)",
        "ANALYZE {{ this }}"
      ]
      +tags: ["core", "memory", "production", "indexed"]
      
    # Semantic associations (materialized table for complex similarity calculations)
    semantic:
      +materialized: table
      +unique_key: [source_concept, target_concept]
      +pre-hook: [
        "{{ strengthen_associations() }}",
        "SET memory_limit = '8GB'",
        "SET enable_hash_join = true"
      ]
      +post-hook: [
        "{{ update_semantic_graph() }}",
        "CREATE INDEX IF NOT EXISTS idx_{{ this.name }}_source_target ON {{ this }} (source_concept, target_concept)",
        "CREATE INDEX IF NOT EXISTS idx_{{ this.name }}_similarity ON {{ this }} (similarity_score DESC)",
        "ANALYZE {{ this }}"
      ]
      +tags: ["semantic", "performance_intensive"]
      
    # Analytics and reporting (views for flexibility with performance hints)
    analytics:
      +materialized: view
      +pre-hook: "SET enable_index_scan = true"
      +tags: ["analytics", "dashboard", "reporting"]
      
    # Insights models (for MVP pipeline)
    insights:
      +materialized: view
      +tags: ["insights", "mvp"]
      
    # Performance models (ephemeral for real-time monitoring)
    performance:
      +materialized: ephemeral
      +pre-hook: "SET threads = 2"
      +tags: ["monitoring", "performance_tracking"]
      
    # Consolidation models (incremental with heavy optimization)
    consolidation:
      +materialized: incremental  
      +unique_key: id
      +incremental_strategy: merge
      +pre-hook: [
        "SET memory_limit = '10GB'",
        "SET threads = 4",
        "SET enable_parallel_aggregation = true"
      ]
      +post-hook: [
        "VACUUM ANALYZE {{ this }}",
        "CREATE INDEX IF NOT EXISTS idx_{{ this.name }}_consolidated_strength ON {{ this }} (consolidated_strength DESC)",
        "DELETE FROM {{ this }} WHERE consolidated_strength < {{ var('weak_connection_threshold') }}"
      ]
      +tags: ["consolidation", "memory_intensive"]

# Snapshot configurations for tracking memory evolution
snapshots:
  biological_memory:
    +target_schema: memory_snapshots
    +unique_key: memory_id
    +strategy: timestamp
    +updated_at: last_modified_at

# Test configurations
tests:
  biological_memory:
    +store_failures: true
    +schema: test_results
